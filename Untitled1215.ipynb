{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a40f47-6f34-4966-9b1d-50d9d2af3b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (11.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.14.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install lxml pillow tqdm numpy h5py pyyaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d06f2d6-cf2d-47d5-a621-ef84c6ffcce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Split sizes: train=534, val=114, test=116, total=764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing train: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 534/534 [00:02<00:00, 250.07it/s]\n",
      "Writing val: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 114/114 [00:01<00:00, 100.90it/s]\n",
      "Writing test: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 323.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] YOLO labels written.\n",
      "[INFO] Class counts: {'helmet': 0, 'no_helmet': 0}\n",
      "[INFO] COCO JSONs written to C:\\Users\\sagni\\Downloads\\Helmet Checker\\coco\n",
      "[INFO] YAML written to C:\\Users\\sagni\\Downloads\\Helmet Checker\\helmet_dataset.yaml\n",
      "[INFO] PKL written to C:\\Users\\sagni\\Downloads\\Helmet Checker\\helmet_metadata.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Packing H5 train: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 534/534 [00:05<00:00, 96.34it/s]\n",
      "Packing H5 val: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 114/114 [00:00<00:00, 708.15it/s]\n",
      "Packing H5 test: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 741.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] HDF5 written to C:\\Users\\sagni\\Downloads\\Helmet Checker\\helmet_dataset.h5\n",
      "\n",
      "[DONE] Outputs created:\n",
      " - YOLO data tree: C:\\Users\\sagni\\Downloads\\Helmet Checker\\data\\(images,labels)\\(train,val,test)\n",
      " - COCO JSONs:     C:\\Users\\sagni\\Downloads\\Helmet Checker\\coco\\coco_annotations_{train|val|test}.json\n",
      " - YAML:           C:\\Users\\sagni\\Downloads\\Helmet Checker\\helmet_dataset.yaml\n",
      " - PKL:            C:\\Users\\sagni\\Downloads\\Helmet Checker\\helmet_metadata.pkl\n",
      " - HDF5:           C:\\Users\\sagni\\Downloads\\Helmet Checker\\helmet_dataset.h5\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, random, json, pickle, io\n",
    "from pathlib import Path\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import h5py\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------\n",
    "# CONFIG — edit if needed\n",
    "# --------------------------\n",
    "IMAGES_DIR = r\"C:\\Users\\sagni\\Downloads\\Helmet Checker\\archive\\images\"\n",
    "ANN_DIR    = r\"C:\\Users\\sagni\\Downloads\\Helmet Checker\\archive\\annotations\"\n",
    "OUT_ROOT   = r\"C:\\Users\\sagni\\Downloads\\Helmet Checker\"\n",
    "\n",
    "# Splits and seed\n",
    "SPLITS = {\"train\": 0.70, \"val\": 0.15, \"test\": 0.15}\n",
    "SEED = 42\n",
    "\n",
    "# Class handling for AndrewMVD (small) dataset:\n",
    "# It typically has 'helmet' and 'head' (we'll map head -> no_helmet).\n",
    "VOC_TO_CANON = {\n",
    "    \"helmet\": \"helmet\",\n",
    "    \"head\": \"no_helmet\",\n",
    "    # Add any unexpected variants here if needed, e.g.:\n",
    "    # \"no-helmet\": \"no_helmet\",\n",
    "}\n",
    "\n",
    "# Canonical ordered class list for YOLO/COCO\n",
    "CLASSES = [\"helmet\", \"no_helmet\"]  # index 0,1\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def list_xmls(ann_dir: Path):\n",
    "    return sorted([p for p in ann_dir.glob(\"*.xml\")])\n",
    "\n",
    "def parse_voc(xml_path: Path):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      img_filename (str),\n",
    "      width (int),\n",
    "      height (int),\n",
    "      annotations: list of dicts {cls_name, xmin, ymin, xmax, ymax}\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    filename = root.findtext(\"filename\")\n",
    "    size = root.find(\"size\")\n",
    "    w = int(size.findtext(\"width\"))\n",
    "    h = int(size.findtext(\"height\"))\n",
    "\n",
    "    anns = []\n",
    "    for obj in root.findall(\"object\"):\n",
    "        name = obj.findtext(\"name\").strip()\n",
    "        bnd = obj.find(\"bndbox\")\n",
    "        xmin = int(float(bnd.findtext(\"xmin\")))\n",
    "        ymin = int(float(bnd.findtext(\"ymin\")))\n",
    "        xmax = int(float(bnd.findtext(\"xmax\")))\n",
    "        ymax = int(float(bnd.findtext(\"ymax\")))\n",
    "        anns.append({\n",
    "            \"cls_name\": name,\n",
    "            \"xmin\": xmin, \"ymin\": ymin, \"xmax\": xmax, \"ymax\": ymax\n",
    "        })\n",
    "    return filename, w, h, anns\n",
    "\n",
    "def canon_class(name: str):\n",
    "    name = name.lower().strip()\n",
    "    return VOC_TO_CANON.get(name, None)\n",
    "\n",
    "def yolo_bbox(xmin, ymin, xmax, ymax, img_w, img_h):\n",
    "    # Convert VOC to YOLO normalized cx, cy, w, h (in [0,1])\n",
    "    cx = (xmin + xmax) / 2.0 / img_w\n",
    "    cy = (ymin + ymax) / 2.0 / img_h\n",
    "    bw = (xmax - xmin) / float(img_w)\n",
    "    bh = (ymax - ymin) / float(img_h)\n",
    "    return cx, cy, bw, bh\n",
    "\n",
    "def coco_bbox(xmin, ymin, xmax, ymax):\n",
    "    # COCO uses [x, y, width, height]\n",
    "    return [xmin, ymin, xmax - xmin, ymax - ymin]\n",
    "\n",
    "# --------------------------\n",
    "# Discover and split\n",
    "# --------------------------\n",
    "random.seed(SEED)\n",
    "images_dir = Path(IMAGES_DIR)\n",
    "ann_dir = Path(ANN_DIR)\n",
    "out_root = Path(OUT_ROOT)\n",
    "\n",
    "xml_files = list_xmls(ann_dir)\n",
    "if not xml_files:\n",
    "    raise RuntimeError(f\"No XML files found in {ann_dir}\")\n",
    "\n",
    "# Align xmls with images; filter any missing\n",
    "records = []\n",
    "missing_images = []\n",
    "for xml in xml_files:\n",
    "    try:\n",
    "        filename, w, h, anns = parse_voc(xml)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to parse {xml}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # image path (handle cases where filename has extension)\n",
    "    img_path = (images_dir / filename)\n",
    "    if not img_path.exists():\n",
    "        # Sometimes filename in xml differs; try same stem with common image extensions\n",
    "        stem = Path(filename).stem\n",
    "        candidates = list(images_dir.glob(stem + \".*\"))\n",
    "        if candidates:\n",
    "            img_path = candidates[0]\n",
    "        else:\n",
    "            missing_images.append(filename)\n",
    "            continue\n",
    "\n",
    "    # Map classes & drop unknown\n",
    "    mapped = []\n",
    "    for a in anns:\n",
    "        c = canon_class(a[\"cls_name\"])\n",
    "        if c in CLASSES:\n",
    "            mapped.append({**a, \"cls_name\": c})\n",
    "        else:\n",
    "            # Unknown class -> skip object\n",
    "            pass\n",
    "\n",
    "    # It's OK if an image ends with 0 valid objects, but we keep it for completeness\n",
    "    records.append({\n",
    "        \"xml\": xml,\n",
    "        \"img\": img_path,\n",
    "        \"width\": w,\n",
    "        \"height\": h,\n",
    "        \"anns\": mapped\n",
    "    })\n",
    "\n",
    "if missing_images:\n",
    "    print(f\"[WARN] {len(missing_images)} annotation(s) skipped due to missing image files.\")\n",
    "\n",
    "# Shuffle and split\n",
    "random.shuffle(records)\n",
    "n = len(records)\n",
    "n_train = int(SPLITS[\"train\"] * n)\n",
    "n_val = int(SPLITS[\"val\"] * n)\n",
    "n_test = n - n_train - n_val\n",
    "splits = {\n",
    "    \"train\": records[:n_train],\n",
    "    \"val\": records[n_train:n_train+n_val],\n",
    "    \"test\": records[n_train+n_val:]\n",
    "}\n",
    "print(f\"[INFO] Split sizes: train={len(splits['train'])}, val={len(splits['val'])}, test={len(splits['test'])}, total={n}\")\n",
    "\n",
    "# --------------------------\n",
    "# Prepare output dirs\n",
    "# --------------------------\n",
    "images_out = {\n",
    "    s: out_root / \"data\" / \"images\" / s for s in splits.keys()\n",
    "}\n",
    "labels_out = {\n",
    "    s: out_root / \"data\" / \"labels\" / s for s in splits.keys()\n",
    "}\n",
    "for d in list(images_out.values()) + list(labels_out.values()):\n",
    "    ensure_dir(d)\n",
    "\n",
    "# --------------------------\n",
    "# Write YOLO labels + copy images\n",
    "# --------------------------\n",
    "cls_to_id = {c: i for i, c in enumerate(CLASSES)}\n",
    "split_counts = {s: 0 for s in splits}\n",
    "class_counts = {c: 0 for c in CLASSES}\n",
    "\n",
    "for s, recs in splits.items():\n",
    "    for rec in tqdm(recs, desc=f\"Writing {s}\"):\n",
    "        img_src = rec[\"img\"]\n",
    "        img_name = img_src.name\n",
    "        label_stem = Path(img_name).stem\n",
    "\n",
    "        # Copy image\n",
    "        shutil.copy2(img_src, images_out[s] / img_name)\n",
    "\n",
    "        # Write YOLO label\n",
    "        yolo_lines = []\n",
    "        for a in rec[\"anns\"]:\n",
    "            cls_id = cls_to_id[a[\"cls_name\"]]\n",
    "            cx, cy, bw, bh = yolo_bbox(a[\"xmin\"], a[\"ymin\"], a[\"xmax\"], a[\"ymax\"], rec[\"width\"], rec[\"height\"])\n",
    "            yolo_lines.append(f\"{cls_id} {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\")\n",
    "            class_counts[a[\"cls_name\"]] += 1\n",
    "\n",
    "        with open(labels_out[s] / f\"{label_stem}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(yolo_lines))\n",
    "\n",
    "        split_counts[s] += 1\n",
    "\n",
    "print(\"[INFO] YOLO labels written.\")\n",
    "print(\"[INFO] Class counts:\", class_counts)\n",
    "\n",
    "# --------------------------\n",
    "# COCO JSON per split\n",
    "# --------------------------\n",
    "def coco_for_split(name, recs, out_path: Path):\n",
    "    coco = {\n",
    "        \"info\": {\"description\": f\"Helmet dataset ({name})\", \"version\": \"1.0\"},\n",
    "        \"licenses\": [],\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [{\"id\": i, \"name\": c, \"supercategory\": \"object\"} for i, c in enumerate(CLASSES)]\n",
    "    }\n",
    "    ann_id = 1\n",
    "    for img_id, rec in enumerate(recs, start=1):\n",
    "        img_name = rec[\"img\"].name\n",
    "        coco[\"images\"].append({\n",
    "            \"id\": img_id,\n",
    "            \"file_name\": img_name,\n",
    "            \"width\": rec[\"width\"],\n",
    "            \"height\": rec[\"height\"]\n",
    "        })\n",
    "        for a in rec[\"anns\"]:\n",
    "            cls_id = cls_to_id[a[\"cls_name\"]]\n",
    "            bbox = coco_bbox(a[\"xmin\"], a[\"ymin\"], a[\"xmax\"], a[\"ymax\"])\n",
    "            coco[\"annotations\"].append({\n",
    "                \"id\": ann_id,\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": cls_id,\n",
    "                \"bbox\": bbox,\n",
    "                \"area\": bbox[2]*bbox[3],\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            ann_id += 1\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(coco, f, indent=2)\n",
    "\n",
    "coco_dir = out_root / \"coco\"\n",
    "ensure_dir(coco_dir)\n",
    "for s, recs in splits.items():\n",
    "    coco_for_split(s, recs, coco_dir / f\"coco_annotations_{s}.json\")\n",
    "print(\"[INFO] COCO JSONs written to\", coco_dir)\n",
    "\n",
    "# --------------------------\n",
    "# YAML (Ultralytics) dataset file\n",
    "# --------------------------\n",
    "yaml_path = out_root / \"helmet_dataset.yaml\"\n",
    "yaml_data = {\n",
    "    \"path\": str((out_root / \"data\").resolve()),\n",
    "    \"train\": \"images/train\",\n",
    "    \"val\": \"images/val\",\n",
    "    \"test\": \"images/test\",\n",
    "    \"names\": CLASSES\n",
    "}\n",
    "with open(yaml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.safe_dump(yaml_data, f, sort_keys=False)\n",
    "print(\"[INFO] YAML written to\", yaml_path)\n",
    "\n",
    "# --------------------------\n",
    "# PKL metadata\n",
    "# --------------------------\n",
    "meta = {\n",
    "    \"classes\": CLASSES,\n",
    "    \"class_to_id\": cls_to_id,\n",
    "    \"splits_counts\": split_counts,\n",
    "    \"class_counts\": class_counts,\n",
    "    \"seed\": SEED,\n",
    "    \"source_images_dir\": str(images_dir),\n",
    "    \"source_annotations_dir\": str(ann_dir),\n",
    "    \"notes\": \"head mapped to no_helmet via VOC_TO_CANON.\"\n",
    "}\n",
    "pkl_path = out_root / \"helmet_metadata.pkl\"\n",
    "with open(pkl_path, \"wb\") as f:\n",
    "    pickle.dump(meta, f)\n",
    "print(\"[INFO] PKL written to\", pkl_path)\n",
    "\n",
    "# --------------------------\n",
    "# HDF5 pack (images + anns)\n",
    "# --------------------------\n",
    "# Store per-split datasets:\n",
    "#   /train/images (N, ) bytes of encoded images\n",
    "#   /train/labels  JSON strings per image with list of {cls_id, bbox[xmin,ymin,xmax,ymax]}\n",
    "#   similarly for val/test\n",
    "h5_path = out_root / \"helmet_dataset.h5\"\n",
    "with h5py.File(h5_path, \"w\") as h5:\n",
    "    for s, recs in splits.items():\n",
    "        grp = h5.create_group(s)\n",
    "        img_bytes_ds = grp.create_dataset(\"images\", (len(recs),), dtype=h5py.special_dtype(vlen=np.dtype('uint8')))\n",
    "        labels_ds = grp.create_dataset(\"labels_json\", (len(recs),), dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "        names_ds = grp.create_dataset(\"image_names\", (len(recs),), dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "\n",
    "        for i, rec in enumerate(tqdm(recs, desc=f\"Packing H5 {s}\")):\n",
    "            # Read and store encoded bytes (keep original format)\n",
    "            with open(rec[\"img\"], \"rb\") as f:\n",
    "                data = f.read()\n",
    "            img_bytes = np.frombuffer(data, dtype=np.uint8)\n",
    "            img_bytes_ds[i] = img_bytes\n",
    "\n",
    "            names_ds[i] = rec[\"img\"].name\n",
    "\n",
    "            # Labels as JSON (cls_id + VOC bbox for fidelity)\n",
    "            lab = []\n",
    "            for a in rec[\"anns\"]:\n",
    "                lab.append({\n",
    "                    \"cls_id\": cls_to_id[a[\"cls_name\"]],\n",
    "                    \"bbox_voc\": [a[\"xmin\"], a[\"ymin\"], a[\"xmax\"], a[\"ymax\"]],\n",
    "                })\n",
    "            labels_ds[i] = json.dumps(lab)\n",
    "\n",
    "print(\"[INFO] HDF5 written to\", h5_path)\n",
    "\n",
    "print(\"\\n[DONE] Outputs created:\")\n",
    "print(f\" - YOLO data tree: {out_root}\\\\data\\\\(images,labels)\\\\(train,val,test)\")\n",
    "print(f\" - COCO JSONs:     {coco_dir}\\\\coco_annotations_{{train|val|test}}.json\")\n",
    "print(f\" - YAML:           {yaml_path}\")\n",
    "print(f\" - PKL:            {pkl_path}\")\n",
    "print(f\" - HDF5:           {h5_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff0ea3-0c5b-4f1f-a0e8-61b6f2ea262f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
